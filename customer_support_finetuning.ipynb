{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Support LLM Fine-Tuning (QLoRA)\n",
        "\n",
        "This notebook walks through the full pipeline from **main.py**: loading the Bitext Customer Support dataset, preparing it for chat-style training, fine-tuning **Llama 3.2 3B** with **QLoRA**, evaluating the model, and running a Gradio demo.\n",
        "\n",
        "## Overview\n",
        "\n",
        "| Item | Value |\n",
        "|------|-------|\n",
        "| **Base model** | meta-llama/Llama-3.2-3B |\n",
        "| **Technique** | QLoRA (4-bit + LoRA) or float16/bf16 + LoRA on Mac |\n",
        "| **Dataset** | Bitext Customer Support (Hugging Face) |\n",
        "| **Output** | Adapter weights + tokenizer in `output_dir` |\n",
        "\n",
        "## Notebook structure\n",
        "\n",
        "1. **Setup** – Install/import dependencies and set paths.\n",
        "2. **Load & explore data** – Load the dataset and inspect schema and samples.\n",
        "3. **Prepare dataset** – Format as chat (system / user / assistant) and split train/eval.\n",
        "4. **Train** – Run QLoRA fine-tuning (optionally on a subset for quick runs).\n",
        "5. **Evaluate** – Compare base vs fine-tuned on custom prompts and save results.\n",
        "6. **Demo** – Launch Gradio chat UI with the fine-tuned adapter or **Ollama** (localhost).\n",
        "\n",
        "Set `USE_OLLAMA = True` and run `ollama serve` + `ollama pull llama3.2` to use local Ollama for evaluate & demo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup\n",
        "\n",
        "Ensure dependencies are installed (`pip install -r requirements.txt`). We import from **main.py** so the notebook stays in sync with the script. Run this cell first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies if running in a fresh environment (e.g. Colab)\n",
        "# !pip install -q torch transformers datasets accelerate bitsandbytes peft trl huggingface-hub gradio\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root so we can import main\n",
        "ROOT = Path.cwd()\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "import torch\n",
        "from main import (\n",
        "    DATASET_NAME,\n",
        "    MODEL_NAME,\n",
        "    TRAINING_CONFIG,\n",
        "    TEST_PROMPTS,\n",
        "    OLLAMA_MODEL_DEFAULT,\n",
        "    get_ollama_models,\n",
        "    ensure_ollama_model,\n",
        "    prepare_dataset,\n",
        "    train,\n",
        "    evaluate,\n",
        "    run_demo,\n",
        "    load_model_for_inference,\n",
        "    generate_response,\n",
        "    generate_response_ollama,\n",
        "    generate_evaluation_report,\n",
        ")\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "try:\n",
        "    print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "except AttributeError:\n",
        "    print(\"MPS: N/A\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration (edit these for your run)\n",
        "\n",
        "- **SUBSET_SIZE**: `None` = full dataset; `500` = quick test on 500 train + 50 eval.\n",
        "- **OUTPUT_DIR**: Where the adapter and tokenizer are saved after training.\n",
        "- **ADAPTER_PATH**: Path to load for evaluation and demo (when not using Ollama).\n",
        "- **USE_OLLAMA**: `True` = use Ollama on localhost for evaluate & demo (no Hugging Face model load).\n",
        "- **OLLAMA_MODEL**: Model name in Ollama (default `gemma3:12b`). Pull with: `ollama pull gemma3:12b`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SUBSET_SIZE = 500          # None for full dataset; 500 for quick test\n",
        "OUTPUT_DIR = \"output/customer-support-llm\"\n",
        "ADAPTER_PATH = \"output/customer-support-llm\"   # Use after training, or your own path\n",
        "EVAL_OUTPUT_JSON = \"evaluation_results.json\"\n",
        "\n",
        "# Use Ollama (localhost) for inference instead of Hugging Face model\n",
        "USE_OLLAMA = True\n",
        "OLLAMA_MODEL = \"gemma3:12b\"   # Local model; pull with: ollama pull gemma3:12b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# When using Ollama: list local models and verify the selected one is available\n",
        "if USE_OLLAMA:\n",
        "    models = get_ollama_models()\n",
        "    if models:\n",
        "        print(\"Locally available Ollama models:\", models)\n",
        "        ensure_ollama_model(OLLAMA_MODEL)\n",
        "        print(f\"Using: {OLLAMA_MODEL}\")\n",
        "    else:\n",
        "        print(\"Ollama not running or no models. Start: ollama serve && ollama pull gemma3:12b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Load and explore the dataset\n",
        "\n",
        "The **Bitext Customer Support** dataset has one split (`train`) with columns: `instruction` (customer query), `response` (agent reply), `category`, `intent`, `flags`. We load it and show a few rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw = load_dataset(DATASET_NAME)\n",
        "ds_train = raw[\"train\"]\n",
        "\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Total rows: {len(ds_train)}\")\n",
        "print(f\"Columns: {ds_train.column_names}\")\n",
        "print(f\"Features: {ds_train.features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample rows\n",
        "for i in [0, 1, 2]:\n",
        "    row = ds_train[i]\n",
        "    print(f\"--- Example {i} ---\")\n",
        "    print(f\"Instruction: {row['instruction'][:120]}...\" if len(row['instruction']) > 120 else f\"Instruction: {row['instruction']}\")\n",
        "    print(f\"Response (first 150 chars): {row['response'][:150]}...\")\n",
        "    print(f\"Category: {row['category']}, Intent: {row['intent']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Prepare dataset for training\n",
        "\n",
        "We convert each example into a **chat** format with three roles:\n",
        "- **system**: \"You are a helpful customer support assistant.\"\n",
        "- **user**: the `instruction` (customer query)\n",
        "- **assistant**: the `response` (target reply)\n",
        "\n",
        "Then we split into train/eval (default 95% / 5%). Optionally we use a **subset** for fast iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds, eval_ds = prepare_dataset(\n",
        "    subset_size=SUBSET_SIZE,\n",
        "    seed=TRAINING_CONFIG[\"seed\"],\n",
        "    test_size=TRAINING_CONFIG[\"test_size\"],\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_ds)}, Eval size: {len(eval_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect one formatted example (what SFTTrainer will see)\n",
        "ex = train_ds[0]\n",
        "print(\"Keys:\", ex.keys())\n",
        "for msg in ex[\"messages\"]:\n",
        "    print(f\"  {msg['role']}: {msg['content'][:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Train the model (QLoRA)\n",
        "\n",
        "This cell loads the base model (4-bit on CUDA, or float16/bf16 on Mac), attaches LoRA adapters, and runs **SFTTrainer** with the config from **main.py** (max length 1024, 1 epoch, etc.).\n",
        "\n",
        "- **With GPU (CUDA)**: Uses QLoRA (4-bit quantization + LoRA).\n",
        "- **On Mac (no CUDA)**: Uses full-precision or float16 + LoRA; batch size is reduced to avoid OOM.\n",
        "\n",
        "Training can take a while; using `SUBSET_SIZE = 500` keeps it short for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    subset_size=SUBSET_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training, the adapter and tokenizer are saved under `OUTPUT_DIR`. You can use this path for evaluation and the demo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation report (HTML)\n",
        "\n",
        "Run the **Evaluate** cell (section 5) first, then run the cell below to generate a self-contained HTML report: training stats (LoRA/QLoRA), evaluation metrics (relevance, length), scalability/maintainability notes, and before/after comparison table. Open **evaluation_report.html** in a browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate HTML report (optional: pass training_stats for LoRA/QLoRA stats and loss curve)\n",
        "generate_evaluation_report(\n",
        "    evaluation_path=EVAL_OUTPUT_JSON,\n",
        "    output_html_path=\"evaluation_report.html\",\n",
        "    training_stats_path=f\"{OUTPUT_DIR}/training_stats.json\" if not USE_OLLAMA else None,\n",
        ")\n",
        "# Open evaluation_report.html in your browser to view the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Evaluation\n",
        "\n",
        "We compare **base** vs **fine-tuned** model on 15 custom test prompts (not from the training set). Results are written to a JSON file and a short summary is printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate(\n",
        "    adapter_path=ADAPTER_PATH,\n",
        "    output_path=EVAL_OUTPUT_JSON,\n",
        "    use_ollama=USE_OLLAMA,\n",
        "    ollama_model=OLLAMA_MODEL,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: load and display a few results (JSON has \"results\" + \"metrics\")\n",
        "import json\n",
        "with open(EVAL_OUTPUT_JSON) as f:\n",
        "    data = json.load(f)\n",
        "results = data[\"results\"] if isinstance(data, dict) and \"results\" in data else data\n",
        "if isinstance(data, dict) and data.get(\"metrics\"):\n",
        "    print(\"Metrics:\", data[\"metrics\"])\n",
        "for r in results[:3]:\n",
        "    print(\"Prompt:\", r[\"prompt\"])\n",
        "    if \"ollama_output\" in r:\n",
        "        print(\"Ollama (first 200 chars):\", r[\"ollama_output\"][:200])\n",
        "    else:\n",
        "        print(\"Base (first 200 chars):\", r[\"base_output\"][:200])\n",
        "        print(\"Fine-tuned (first 200 chars):\", r[\"fine_tuned_output\"][:200])\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Interactive demo (Gradio)\n",
        "\n",
        "Launches a web UI: with **Ollama** (`USE_OLLAMA = True`) it uses your local Ollama model; otherwise it loads the Hugging Face adapter. **Full chat history** is sent each turn so multi-turn answers stay correct. Use **Max response tokens** to trade off speed vs length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_demo(\n",
        "    adapter_path=ADAPTER_PATH,\n",
        "    use_ollama=USE_OLLAMA,\n",
        "    ollama_model=OLLAMA_MODEL,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: multi-turn inference with chat history\n",
        "\n",
        "Same as the demo: pass **chat_history** so the model sees the full conversation. Example with Ollama:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: two turns with Ollama (full history sent each time)\n",
        "if USE_OLLAMA:\n",
        "    history = []\n",
        "    for user_msg in [\"I want to cancel my order #12345.\", \"What do I do next?\"]:\n",
        "        reply = generate_response_ollama(\n",
        "            user_msg,\n",
        "            model_name=OLLAMA_MODEL,\n",
        "            chat_history=history,\n",
        "            max_tokens=128,\n",
        "        )\n",
        "        history.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "        print(f\"User: {user_msg}\")\n",
        "        print(f\"Assistant: {reply[:200]}...\")\n",
        "        print()\n",
        "# With Hugging Face: use generate_response(..., chat_history=history) the same way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "| Step | What it does |\n",
        "|------|----------------|\n",
        "| **main.py** | CLI: `train`, `evaluate`, `demo` with the same logic as this notebook. |\n",
        "| **Data** | Bitext → chat format (system/user/assistant) → train/eval split. |\n",
        "| **Train** | QLoRA (or LoRA on Mac) with SFTTrainer; adapter saved to `OUTPUT_DIR`. |\n",
        "| **Evaluate** | Base vs fine-tuned on 15 prompts → `evaluation_results.json` (with metrics). |\n",
        "| **Report** | `report --evaluation ... --training_stats ...` → HTML evaluation report. |\n",
        "| **Demo** | Gradio UI; full chat history sent each turn (Ollama or HF). |\n",
        "\n",
        "For more detail, see **PLAN.md**, **Instructions.md**, and **cutomer-support-plan.txt**."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
